# Self-Attention and Softmax

This experiment manually reproduces the core idea of self-attention
used in Transformer models.

Starting from token embeddings produced by a pretrained encoder
(DistilBERT), the following steps are illustrated:

- Token embeddings represent input tokens without context.
- Query (Q), Key (K), and Value (V) vectors are derived from embeddings
  (here simplified as identity mappings for clarity).
- Attention scores are computed using scaled dot-product attention:
  
  Q · Kᵀ / √d

- Softmax is applied row-wise to obtain attention weights.
  Each row sums to 1.0 and represents how much a token attends to others.

- The final attention output is a weighted sum of Value vectors,
  producing contextualized token embeddings.

Key observations:
- Attention operates at the **token level**, not the sentence level.
- Attention does not create meaning by itself; it redistributes information
  across tokens based on learned relationships.
- Sentence embeddings used for retrieval are produced **after**
  attention, via pooling or specialized embedding models.

This experiment focuses on understanding the mechanism,
not on reproducing the full multi-head attention used in production models.

## Multi-Head Attention

Multi-head attention runs several independent attention mechanisms
in parallel on different subspaces of the embedding.

Key ideas:
- Each head attends to tokens independently.
- Each head operates on a reduced dimensionality (d_head).
- Heads are concatenated, not averaged.
- A linear projection is usually applied after concatenation
  (omitted here for clarity).

Why this matters:
- Different heads can focus on different relationships
  (syntax, proximity, emphasis).
- Multi-head attention increases expressiveness
  without increasing sequence length.

Important:
- Multi-head attention still operates at the token level.
- Sentence-level meaning emerges later through pooling or specialized models.
